# -*- coding: utf-8 -*-
"""Emotion Recognition in Arabic Texts.ipynb

Automatically generated by Colab.

"""

import re
import string


#Normalization


!pip install emoji==1.7
import emoji


def is_emoji(s):
  emojis=''
  for c in s:
    if c in emoji.UNICODE_EMOJI['en'] and c not in emojis:
      emojis=emojis+c
  # return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'] and c )
  return emojis

def remove_emojis(data):
    emoj = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002500-\U00002BEF"  # chinese char
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        u"\u2640-\u2642"
        u"\u2600-\u2B55"
        u"\u200d"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\ufe0f"  # dingbats
        u"\u3030"
                      "]+", re.UNICODE)
    return re.sub(emoj, '', data)
  
def is_emoji(s):
  emojis=''
  for c in s:
    if c in emoji.UNICODE_EMOJI['en'] and c not in emojis:
      emojis=emojis+c
  # return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'] and c )
  return emojis

def remove_emoji(text):
    #removing Imojis:
    emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                              "]+", flags=re.UNICODE)
    without_emoji=emoji_pattern.sub(r'',text)

    emojis=''
    for c in text:
      if c in emoji.UNICODE_EMOJI['en'] and c not in emojis:
       emojis=emojis+c
    return without_emoji,emojis


def  remove_id(text):
    #Removing IDs:
    without_id = re.sub('@[^\s]+','',text)
    return without_id
    #Finding Hashtags in caption of post:
    # tags=[]
    # for tag in without_id.split():
    #   if tag.startswith("#"):
    #     tags.append(tag.strip("#").replace("_", " "))

    #Removing # sign and _:

def remove_hashtags(text):
    without_tag=text.replace('#','').replace("_", " ")
    return  without_tag

def remove_links(text):
    link = re.compile(r'https?://\S+|www\.\S+')
    text_without_link = link.sub('', text)

    return text_without_link

def remove_digits(text):
  # special_digits=['0','20','100']
  non_digit=''
  for i in text:
    if not i.isdigit():
      non_digit=non_digit+i
    # elif i in special_digits:
    #   non_digit=non_digit+i
  return non_digit

def remove_html_tags(text):
  cleaner = re.compile('<.*?>')
  without_tag = re.sub(cleaner, '', text)
  result = without_tag.replace(u'\xa0', u' ')
  return result

arabic_punctuations = '''`÷×«»؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ'''
english_punctuations = string.punctuation
punctuations_list = arabic_punctuations + english_punctuations

arabic_diacritics = re.compile("""
                             ّ    | # Tashdid
                             َ    | # Fatha
                             ً    | # Tanwin Fath
                             ُ    | # Damma
                             ٌ    | # Tanwin Damm
                             ِ    | # Kasra
                             ٍ    | # Tanwin Kasr
                             ْ    | # Sukun
                             ـ     # Tatwil/Kashida

                         """, re.VERBOSE)


def normalize_arabic(text):
    text = re.sub("[إأآا]", "ا", text)
    text = re.sub("ى", "ي", text)
    text = re.sub("ؤ", "ء", text)
    text = re.sub("ئ", "ء", text)
    text = re.sub("ة", "ه", text)
    text = re.sub("گ", "ك", text)
    return text


def remove_diacritics(text):
    text = re.sub(arabic_diacritics, '', text)
    return text


def remove_punctuations(text):
    translator = str.maketrans('', '', punctuations_list)
    return text.translate(translator)

# Investigating:
def remove_repeating_char(text):
    return re.sub(r'(.)\1+', r'\1', text)

#Removing  elongation:
def remove_elongation(text):
    text = re.sub('ـ', '', text)
    return text

# Removing Unicodes:
def remove_unicode(text):
    text.encode('ascii', errors='ignore')
    return text

def normalizing(text):
  without_emoji=remove_emojis(text)
  without_id=remove_id(without_emoji)
  without_hashtag=remove_hashtags(without_id)
  without_html_tags=remove_html_tags(without_hashtag)
  without_link=remove_links(without_html_tags)
  without_digits=remove_digits(without_link)
  without_diacritics= remove_diacritics(without_digits)
  without_punctuation=remove_punctuations(without_diacritics)
  without_elongation=remove_elongation(without_punctuation)
  without_unicode=remove_unicode(without_elongation)
  normalized=normalize_arabic(without_unicode)
  return normalized

#Spell Checking:
#need to investigating other models
!pip install ar-corrector
from ar_corrector.corrector import Corrector

corrector = Corrector()
text = '🔻بالصور وقفة في كفرياسيف بالداخل المحتل، تنديداً باستمرار عدوان الإحتلال على قطاع غزة. رفح تباد يا امه المليار'
print(corrector.contextual_correct(text))

"""#Removing StopWords:"""

#Removing Stop words:
# from nltk.corpus import stopwords
import nltk
# nltk.download('stopwords')
nltk.download('punkt')
from textblob import TextBlob

path_of_stopwords=''
with open(path_of_stopwords,'r') as file2:
  stopwords=[]
  line2=file2.readline().replace('\n','')
  stopwords.append(line2)

  while line2:
    line2=file2.readline().replace('\n','')
    stopwords.append(line2)

stopwords.remove('')
stopwords_set = set(stopwords)

def remove_stop_words(text):
    zen = TextBlob(text)
    words = zen.words
    return " ".join([w for w in words if not w in stopwords_set and len(w) >= 2])

without_stopwords=remove_stop_words(preprocessing('أيّها الشباب الجامعيّون في أمريكا!قد شرعتم بنضالٍ شريفٍ تحت ضغوط حكومتكم القاسية، التي تدافع عن الكيان الصــــ.ـ.هيوني. الإمام السيد علي الخامنئي (دام ظله) في رسالة إلى الطلّاب الجامعيّين والشباب في الولايات المتّحدة الأمريكيّة 25/05/2024 https://t.co/4W6wtVpjFw '))

"""#Tokenization:

"""

!pip install pyarabic

from pyarabic.araby import tokenize, is_arabicrange
def tokenize_non_arabic(text):
  token=tokenize(text, conditions=is_arabicrange)
  return token

# def remove_stop_words2(token):
#     t_r=[]
#     # zen = TextBlob(text)
#     # words = zen.words
#     for w in token:
#        if w not in stopwords_set and len(w) >= 2:
#         t_r.append(w)
#     return t_r

#finding and removing non-arabic texts:
def remove_non_arabic(text):
  arabic_text=''
  for ele in text:
    if re.findall("[^\u0000-\u05C0\u2100-\u214F]+", ele):
      arabic_text=arabic_text+ele
  return arabic_text

#!pip install camel_tools
#from camel_tools.tokenizers.morphological import MorphologicalTokenizer
#from camel_tools.tokenizers.word import simple_word_tokenize
#from camel_tools.disambig.mle import MLEDisambiguator

# Initialize MorphologicalTokenizer
#mle = MLEDisambiguator.pretrained('calima-msa-r13')
#tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True)
#def tokenize(sentence):
    # Tokenize the sentence
    #sentence = simple_word_tokenize(sentence)
    #tokens = tokenizer.tokenize(sentence)
    #return tokens

"""#POS_tagger:"""

import nltk
nltk.download('averaged_perceptron_tagger')
from nltk.tag import pos_tag
#stanford is better than nltk:
import nltk.tag.stanford as st
#  import POSTagger
path_to_model='/content/stanford-postagger-full-2020-11-17/models/arabic.tagger'
# artagger = POSTagger(path_to_model, encoding='utf8')
tagger = st.StanfordPOSTagger(path_to_model, '/content/stanford-postagger-full-2020-11-17/stanford-postagger.jar')

tagger._SEPARATOR = '/'
tagged_sent = tagger.tag(tokens)
tagged_sent

pos_tag(tokens)

"""#Stemming:

ISRIStemmer--> root good for verbs<br>
snowball stememr--> light stem for nouns<br>
PorterStemmer-->not for arabic
light stemmer--> best
"""

from nltk import ISRIStemmer
def stemming(tokens):
    stemmer = ISRIStemmer()
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    return stemmed_tokens
stemming(tokens)

"""SnowballStemmer"""

# import nltk
# from nltk.stem.snowball import SnowballStemmer
# snow_stemmer = SnowballStemmer(language='arabic')

stem_words = []
for w in tokens:
    x = snow_stemmer.stem(w)
    stem_words.append(x)

for e1,e2 in zip(tokens,stem_words):
    print(e1+' ----> '+e2)

"""light_stemmer"""

pip install tashaphyne

from tashaphyne.stemming import ArabicLightStemmer
ArListem = ArabicLightStemmer()
def light_stemmer(tokens):
  stem_words = []
  for w in tokens:
      stem = ArListem.light_stem(w)
      stem_words.append(stem)
  return stem_words

light_stemmer(tokens)

text='🔻بالصور وقفة في كفرياسيف بالداخل المحتل، تنديداً باستمرار عدوان الإحتلال على قطاع غزة. #رفح_تباد_يا_امه_المليار #رفح_تُباد_يا_أمة_المليار https://t.co/jh9CHDncpm '
text=remove_stop_words(normalizing(text))
tokens=tokenize(text)

text='بالصور وقفه كفرياسيف بالداخل المحتل تنديدا باستمرار عدوان الاحتلال قطاع غزه رفح تباد امه المليار رفح تباد امه المليار'

tokens=['بالصور','وقفه','كفرياسيف','بالداخل','المحتل','تنديدا','باستمرار','عدوان','الاحتلال','قطاع','غزه','رفح','تباد','امه','المليار','رفح','تباد','امه','المليار']

"""farasa"""

from farasa.stemmer import FarasaStemmer

stemmer = FarasaStemmer()

stemmer_interactive = FarasaStemmer(interactive=True)

stemmed_interactive = stemmer_interactive.stem(text)
print("sample stemmed (interactive):",stemmed_interactive)
# terminate the object to save resources:
# stemmer_interactive.terminate()

"""#Other materials:

##Farasa NER:
"""

pip install farasapy

from farasa.ner import FarasaNamedEntityRecognizer
named_entity_recognizer = FarasaNamedEntityRecognizer()

named_entity_recognized = named_entity_recognizer.recognize('غزه')
print("sample named entity recognized:",named_entity_recognized)

def ner(token):
  named_entity_recognized = named_entity_recognizer.recognize(token)
  return named_entity_recognized

def stemmer(tokens):
  stem_list=[]
  entities=[]
  for token in tokens:
    if ner(token) in entities:
      stem_list.append(token)
    else:
      stem_list.append(stem(token))

"""##Farasa Stemmer: take long time, more accurate(5%)"""

from farasa.stemmer import FarasaStemmer

stemmer = FarasaStemmer()

stemmer.stem("مليار" )
