# -*- coding: utf-8 -*-
"""Emotion Recognition in Arabic Texts.ipynb

Automatically generated by Colab.

"""

import re
import string


#Normalization


!pip install emoji==1.7
import emoji


def is_emoji(s):
  emojis=''
  for c in s:
    if c in emoji.UNICODE_EMOJI['en'] and c not in emojis:
      emojis=emojis+c
  # return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'] and c )
  return emojis

def remove_emojis(data):
    emoj = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002500-\U00002BEF"  # chinese char
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        u"\u2640-\u2642"
        u"\u2600-\u2B55"
        u"\u200d"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\ufe0f"  # dingbats
        u"\u3030"
                      "]+", re.UNICODE)
    return re.sub(emoj, '', data)
  
def is_emoji(s):
  emojis=''
  for c in s:
    if c in emoji.UNICODE_EMOJI['en'] and c not in emojis:
      emojis=emojis+c
  # return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'] and c )
  return emojis

def remove_emoji(text):
    #removing Imojis:
    emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                              "]+", flags=re.UNICODE)
    without_emoji=emoji_pattern.sub(r'',text)

    emojis=''
    for c in text:
      if c in emoji.UNICODE_EMOJI['en'] and c not in emojis:
       emojis=emojis+c
    return without_emoji,emojis


def  remove_id(text):
    #Removing IDs:
    without_id = re.sub('@[^\s]+','',text)
    return without_id
    #Finding Hashtags in caption of post:
    # tags=[]
    # for tag in without_id.split():
    #   if tag.startswith("#"):
    #     tags.append(tag.strip("#").replace("_", " "))

    #Removing # sign and _:

def remove_hashtags(text):
    without_tag=text.replace('#','').replace("_", " ")
    return  without_tag

def remove_links(text):
    link = re.compile(r'https?://\S+|www\.\S+')
    text_without_link = link.sub('', text)

    return text_without_link

def remove_digits(text):
  # special_digits=['0','20','100']
  non_digit=''
  for i in text:
    if not i.isdigit():
      non_digit=non_digit+i
    # elif i in special_digits:
    #   non_digit=non_digit+i
  return non_digit

def remove_html_tags(text):
  cleaner = re.compile('<.*?>')
  without_tag = re.sub(cleaner, '', text)
  result = without_tag.replace(u'\xa0', u' ')
  return result

arabic_punctuations = '''`Ã·Ã—Â«Â»Ø›<>_()*&^%][Ù€ØŒ/:"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€'''
english_punctuations = string.punctuation
punctuations_list = arabic_punctuations + english_punctuations

arabic_diacritics = re.compile("""
                             Ù‘    | # Tashdid
                             Ù    | # Fatha
                             Ù‹    | # Tanwin Fath
                             Ù    | # Damma
                             ÙŒ    | # Tanwin Damm
                             Ù    | # Kasra
                             Ù    | # Tanwin Kasr
                             Ù’    | # Sukun
                             Ù€     # Tatwil/Kashida

                         """, re.VERBOSE)


def normalize_arabic(text):
    text = re.sub("[Ø¥Ø£Ø¢Ø§]", "Ø§", text)
    text = re.sub("Ù‰", "ÙŠ", text)
    text = re.sub("Ø¤", "Ø¡", text)
    text = re.sub("Ø¦", "Ø¡", text)
    text = re.sub("Ø©", "Ù‡", text)
    text = re.sub("Ú¯", "Ùƒ", text)
    return text


def remove_diacritics(text):
    text = re.sub(arabic_diacritics, '', text)
    return text


def remove_punctuations(text):
    translator = str.maketrans('', '', punctuations_list)
    return text.translate(translator)

# Investigating:
def remove_repeating_char(text):
    return re.sub(r'(.)\1+', r'\1', text)

#Removing  elongation:
def remove_elongation(text):
    text = re.sub('Ù€', '', text)
    return text

# Removing Unicodes:
def remove_unicode(text):
    text.encode('ascii', errors='ignore')
    return text

def normalizing(text):
  without_emoji=remove_emojis(text)
  without_id=remove_id(without_emoji)
  without_hashtag=remove_hashtags(without_id)
  without_html_tags=remove_html_tags(without_hashtag)
  without_link=remove_links(without_html_tags)
  without_digits=remove_digits(without_link)
  without_diacritics= remove_diacritics(without_digits)
  without_punctuation=remove_punctuations(without_diacritics)
  without_elongation=remove_elongation(without_punctuation)
  without_unicode=remove_unicode(without_elongation)
  normalized=normalize_arabic(without_unicode)
  return normalized

#Spell Checking:
#need to investigating other models
!pip install ar-corrector
from ar_corrector.corrector import Corrector

corrector = Corrector()
text = 'ğŸ”»Ø¨Ø§Ù„ØµÙˆØ± ÙˆÙ‚ÙØ© ÙÙŠ ÙƒÙØ±ÙŠØ§Ø³ÙŠÙ Ø¨Ø§Ù„Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ„ØŒ ØªÙ†Ø¯ÙŠØ¯Ø§Ù‹ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø¹Ø¯ÙˆØ§Ù† Ø§Ù„Ø¥Ø­ØªÙ„Ø§Ù„ Ø¹Ù„Ù‰ Ù‚Ø·Ø§Ø¹ ØºØ²Ø©. Ø±ÙØ­ ØªØ¨Ø§Ø¯ ÙŠØ§ Ø§Ù…Ù‡ Ø§Ù„Ù…Ù„ÙŠØ§Ø±'
print(corrector.contextual_correct(text))

"""#Removing StopWords:"""

#Removing Stop words:
# from nltk.corpus import stopwords
import nltk
# nltk.download('stopwords')
nltk.download('punkt')
from textblob import TextBlob

path_of_stopwords=''
with open(path_of_stopwords,'r') as file2:
  stopwords=[]
  line2=file2.readline().replace('\n','')
  stopwords.append(line2)

  while line2:
    line2=file2.readline().replace('\n','')
    stopwords.append(line2)

stopwords.remove('')
stopwords_set = set(stopwords)

def remove_stop_words(text):
    zen = TextBlob(text)
    words = zen.words
    return " ".join([w for w in words if not w in stopwords_set and len(w) >= 2])

without_stopwords=remove_stop_words(preprocessing('Ø£ÙŠÙ‘Ù‡Ø§ Ø§Ù„Ø´Ø¨Ø§Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠÙ‘ÙˆÙ† ÙÙŠ Ø£Ù…Ø±ÙŠÙƒØ§!Ù‚Ø¯ Ø´Ø±Ø¹ØªÙ… Ø¨Ù†Ø¶Ø§Ù„Ù Ø´Ø±ÙŠÙÙ ØªØ­Øª Ø¶ØºÙˆØ· Ø­ÙƒÙˆÙ…ØªÙƒÙ… Ø§Ù„Ù‚Ø§Ø³ÙŠØ©ØŒ Ø§Ù„ØªÙŠ ØªØ¯Ø§ÙØ¹ Ø¹Ù† Ø§Ù„ÙƒÙŠØ§Ù† Ø§Ù„ØµÙ€Ù€Ù€Ù€.Ù€.Ù‡ÙŠÙˆÙ†ÙŠ. Ø§Ù„Ø¥Ù…Ø§Ù… Ø§Ù„Ø³ÙŠØ¯ Ø¹Ù„ÙŠ Ø§Ù„Ø®Ø§Ù…Ù†Ø¦ÙŠ (Ø¯Ø§Ù… Ø¸Ù„Ù‡) ÙÙŠ Ø±Ø³Ø§Ù„Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø·Ù„Ù‘Ø§Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠÙ‘ÙŠÙ† ÙˆØ§Ù„Ø´Ø¨Ø§Ø¨ ÙÙŠ Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª Ø§Ù„Ù…ØªÙ‘Ø­Ø¯Ø© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠÙ‘Ø© 25/05/2024 https://t.co/4W6wtVpjFwÂ '))

"""#Tokenization:

"""

!pip install pyarabic

from pyarabic.araby import tokenize, is_arabicrange
def tokenize_non_arabic(text):
  token=tokenize(text, conditions=is_arabicrange)
  return token

# def remove_stop_words2(token):
#     t_r=[]
#     # zen = TextBlob(text)
#     # words = zen.words
#     for w in token:
#        if w not in stopwords_set and len(w) >= 2:
#         t_r.append(w)
#     return t_r

#finding and removing non-arabic texts:
def remove_non_arabic(text):
  arabic_text=''
  for ele in text:
    if re.findall("[^\u0000-\u05C0\u2100-\u214F]+", ele):
      arabic_text=arabic_text+ele
  return arabic_text

#!pip install camel_tools
#from camel_tools.tokenizers.morphological import MorphologicalTokenizer
#from camel_tools.tokenizers.word import simple_word_tokenize
#from camel_tools.disambig.mle import MLEDisambiguator

# Initialize MorphologicalTokenizer
#mle = MLEDisambiguator.pretrained('calima-msa-r13')
#tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True)
#def tokenize(sentence):
    # Tokenize the sentence
    #sentence = simple_word_tokenize(sentence)
    #tokens = tokenizer.tokenize(sentence)
    #return tokens

"""#POS_tagger:"""

import nltk
nltk.download('averaged_perceptron_tagger')
from nltk.tag import pos_tag
#stanford is better than nltk:
import nltk.tag.stanford as st
#  import POSTagger
path_to_model='/content/stanford-postagger-full-2020-11-17/models/arabic.tagger'
# artagger = POSTagger(path_to_model, encoding='utf8')
tagger = st.StanfordPOSTagger(path_to_model, '/content/stanford-postagger-full-2020-11-17/stanford-postagger.jar')

tagger._SEPARATOR = '/'
tagged_sent = tagger.tag(tokens)
tagged_sent

pos_tag(tokens)

"""#Stemming:

ISRIStemmer--> root good for verbs<br>
snowball stememr--> light stem for nouns<br>
PorterStemmer-->not for arabic
light stemmer--> best
"""

from nltk import ISRIStemmer
def stemming(tokens):
    stemmer = ISRIStemmer()
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    return stemmed_tokens
stemming(tokens)

"""SnowballStemmer"""

# import nltk
# from nltk.stem.snowball import SnowballStemmer
# snow_stemmer = SnowballStemmer(language='arabic')

stem_words = []
for w in tokens:
    x = snow_stemmer.stem(w)
    stem_words.append(x)

for e1,e2 in zip(tokens,stem_words):
    print(e1+' ----> '+e2)

"""light_stemmer"""

pip install tashaphyne

from tashaphyne.stemming import ArabicLightStemmer
ArListem = ArabicLightStemmer()
def light_stemmer(tokens):
  stem_words = []
  for w in tokens:
      stem = ArListem.light_stem(w)
      stem_words.append(stem)
  return stem_words

light_stemmer(tokens)

text='ğŸ”»Ø¨Ø§Ù„ØµÙˆØ± ÙˆÙ‚ÙØ© ÙÙŠ ÙƒÙØ±ÙŠØ§Ø³ÙŠÙ Ø¨Ø§Ù„Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ„ØŒ ØªÙ†Ø¯ÙŠØ¯Ø§Ù‹ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø¹Ø¯ÙˆØ§Ù† Ø§Ù„Ø¥Ø­ØªÙ„Ø§Ù„ Ø¹Ù„Ù‰ Ù‚Ø·Ø§Ø¹ ØºØ²Ø©. #Ø±ÙØ­_ØªØ¨Ø§Ø¯_ÙŠØ§_Ø§Ù…Ù‡_Ø§Ù„Ù…Ù„ÙŠØ§Ø± #Ø±ÙØ­_ØªÙØ¨Ø§Ø¯_ÙŠØ§_Ø£Ù…Ø©_Ø§Ù„Ù…Ù„ÙŠØ§Ø± https://t.co/jh9CHDncpmÂ '
text=remove_stop_words(normalizing(text))
tokens=tokenize(text)

text='Ø¨Ø§Ù„ØµÙˆØ± ÙˆÙ‚ÙÙ‡ ÙƒÙØ±ÙŠØ§Ø³ÙŠÙ Ø¨Ø§Ù„Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ„ ØªÙ†Ø¯ÙŠØ¯Ø§ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø¹Ø¯ÙˆØ§Ù† Ø§Ù„Ø§Ø­ØªÙ„Ø§Ù„ Ù‚Ø·Ø§Ø¹ ØºØ²Ù‡ Ø±ÙØ­ ØªØ¨Ø§Ø¯ Ø§Ù…Ù‡ Ø§Ù„Ù…Ù„ÙŠØ§Ø± Ø±ÙØ­ ØªØ¨Ø§Ø¯ Ø§Ù…Ù‡ Ø§Ù„Ù…Ù„ÙŠØ§Ø±'

tokens=['Ø¨Ø§Ù„ØµÙˆØ±','ÙˆÙ‚ÙÙ‡','ÙƒÙØ±ÙŠØ§Ø³ÙŠÙ','Ø¨Ø§Ù„Ø¯Ø§Ø®Ù„','Ø§Ù„Ù…Ø­ØªÙ„','ØªÙ†Ø¯ÙŠØ¯Ø§','Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø±','Ø¹Ø¯ÙˆØ§Ù†','Ø§Ù„Ø§Ø­ØªÙ„Ø§Ù„','Ù‚Ø·Ø§Ø¹','ØºØ²Ù‡','Ø±ÙØ­','ØªØ¨Ø§Ø¯','Ø§Ù…Ù‡','Ø§Ù„Ù…Ù„ÙŠØ§Ø±','Ø±ÙØ­','ØªØ¨Ø§Ø¯','Ø§Ù…Ù‡','Ø§Ù„Ù…Ù„ÙŠØ§Ø±']

"""farasa"""

from farasa.stemmer import FarasaStemmer

stemmer = FarasaStemmer()

stemmer_interactive = FarasaStemmer(interactive=True)

stemmed_interactive = stemmer_interactive.stem(text)
print("sample stemmed (interactive):",stemmed_interactive)
# terminate the object to save resources:
# stemmer_interactive.terminate()

"""#Other materials:

##Farasa NER:
"""

pip install farasapy

from farasa.ner import FarasaNamedEntityRecognizer
named_entity_recognizer = FarasaNamedEntityRecognizer()

named_entity_recognized = named_entity_recognizer.recognize('ØºØ²Ù‡')
print("sample named entity recognized:",named_entity_recognized)

def ner(token):
  named_entity_recognized = named_entity_recognizer.recognize(token)
  return named_entity_recognized

def stemmer(tokens):
  stem_list=[]
  entities=[]
  for token in tokens:
    if ner(token) in entities:
      stem_list.append(token)
    else:
      stem_list.append(stem(token))

"""##Farasa Stemmer: take long time, more accurate(5%)"""

from farasa.stemmer import FarasaStemmer

stemmer = FarasaStemmer()

stemmer.stem("Ù…Ù„ÙŠØ§Ø±" )
