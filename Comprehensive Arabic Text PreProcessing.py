# -*- coding: utf-8 -*-
"""Emotion Recognition in Arabic Texts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fpG2vcyh2MjTiRqs3VpPqHSA_llj38Gn
"""

import re
import string

"""should we remove # words? <br>
should we remove english & non-ararbic(chi-jap) words? yes<br>
improve remove_digits() ok<br>
save_emojis: ok<br>
  و ضمایری مانند کم از جمله: ok
  <br>
حذف فاصله

#Normalization
"""

!pip install emoji==1.7
import emoji

def is_emoji(s):
  emojis=''
  for c in s:
    if c in emoji.UNICODE_EMOJI['en'] and c not in emojis:
      emojis=emojis+c
  # return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'] and c )
  return emojis

# This function modifies caption text and prepare it for key extraction:
def remove_emoji(text):
    #removing Imojis:
    emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                              "]+", flags=re.UNICODE)
    without_emoji=emoji_pattern.sub(r'',text)

    emojis=''
    for c in text:
      if c in emoji.UNICODE_EMOJI['en'] and c not in emojis:
       emojis=emojis+c
    return without_emoji,emojis


def  remove_id(text):
    #Removing IDs:
    without_id = re.sub('@[^\s]+','',text)
    return without_id
    #Finding Hashtags in caption of post:
    # tags=[]
    # for tag in without_id.split():
    #   if tag.startswith("#"):
    #     tags.append(tag.strip("#").replace("_", " "))

    #Removing # sign and _:

def remove_hashtags(text):
    without_tag=text.replace('#','').replace("_", " ")
    return  without_tag

def remove_links(text):
    link = re.compile(r'https?://\S+|www\.\S+')
    text_without_link = link.sub('', text)

    return text_without_link

def remove_digits(text):
  # special_digits=['0','20','100']
  non_digit=''
  for i in text:
    if not i.isdigit():
      non_digit=non_digit+i
    # elif i in special_digits:
    #   non_digit=non_digit+i
  return non_digit

def remove_html_tags(text):
  cleaner = re.compile('<.*?>')
  without_tag = re.sub(cleaner, '', text)
  result = without_tag.replace(u'\xa0', u' ')
  return result

arabic_punctuations = '''`÷×«»؛<>_()*&^%][ـ،/:"؟.,'{}~¦+|!”…“–ـ'''
english_punctuations = string.punctuation
punctuations_list = arabic_punctuations + english_punctuations

arabic_diacritics = re.compile("""
                             ّ    | # Tashdid
                             َ    | # Fatha
                             ً    | # Tanwin Fath
                             ُ    | # Damma
                             ٌ    | # Tanwin Damm
                             ِ    | # Kasra
                             ٍ    | # Tanwin Kasr
                             ْ    | # Sukun
                             ـ     # Tatwil/Kashida

                         """, re.VERBOSE)


def normalize_arabic(text):
    text = re.sub("[إأآا]", "ا", text)
    text = re.sub("ى", "ي", text)
    text = re.sub("ؤ", "ء", text)
    text = re.sub("ئ", "ء", text)
    text = re.sub("ة", "ه", text)
    text = re.sub("گ", "ك", text)
    return text


def remove_diacritics(text):
    text = re.sub(arabic_diacritics, '', text)
    return text


def remove_punctuations(text):
    translator = str.maketrans('', '', punctuations_list)
    return text.translate(translator)

# Investigating:
def remove_repeating_char(text):
    return re.sub(r'(.)\1+', r'\1', text)
#ham digit ro hazf mikone ham 2 harf tekrari ro

#Removing  elongation:
def remove_elongation(text):
    text = re.sub('ـ', '', text)
    return text

# Removing Unicodes:
def remove_unicode(text):
    text.encode('ascii', errors='ignore')
    return text

def normalizing(text):
  without_emoji,emoji_list=remove_emoji(text)
  without_id=remove_id(without_emoji)
  without_hashtag=remove_hashtags(without_id)
  without_html_tags=remove_html_tags(without_hashtag)
  without_link=remove_links(without_html_tags)
  without_digits=remove_digits(without_link)
  without_diacritics= remove_diacritics(without_digits)
  without_punctuation=remove_punctuations(without_diacritics)
  without_elongation=remove_elongation(without_punctuation)
  without_unicode=remove_unicode(without_elongation)
  normalized=normalize_arabic(without_unicode)
  return normalized

#Spell Checking:
#need to investigating other models
!pip install ar-corrector
from ar_corrector.corrector import Corrector

corrector = Corrector()
text = '🔻بالصور وقفة في كفرياسيف بالداخل المحتل، تنديداً باستمرار عدوان الإحتلال على قطاع غزة. رفح تباد يا امه المليار'
print(corrector.contextual_correct(text))

"""#Removing StopWords:"""

#Removing Stop words:
# from nltk.corpus import stopwords
import nltk
# nltk.download('stopwords')
nltk.download('punkt')
from textblob import TextBlob

with open('/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition/combiation_sw2_nonemotional_normalized&rmTashkeel2','r') as file2:
  stopwords=[]
  line2=file2.readline().replace('\n','')
  stopwords.append(line2)

  while line2:
    line2=file2.readline().replace('\n','')
    stopwords.append(line2)

stopwords.remove('')
stopwords_set = set(stopwords)

def remove_stop_words(text):
    zen = TextBlob(text)
    words = zen.words
    return " ".join([w for w in words if not w in stopwords_set and len(w) >= 2])

without_stopwords=remove_stop_words(preprocessing('أيّها الشباب الجامعيّون في أمريكا!قد شرعتم بنضالٍ شريفٍ تحت ضغوط حكومتكم القاسية، التي تدافع عن الكيان الصــــ.ـ.هيوني. الإمام السيد علي الخامنئي (دام ظله) في رسالة إلى الطلّاب الجامعيّين والشباب في الولايات المتّحدة الأمريكيّة 25/05/2024 https://t.co/4W6wtVpjFw '))

stopwords_set = set(stopwords)
stopwords2_set = set(st_t_norm)
in_second_but_not_in_first = stopwords2_set - stopwords_set
result = stopwords + list(in_second_but_not_in_first)
len(result)

f=open('/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition/combiation_sw2_nonemotional_normalized&rmTashkeel2','w')
for i in range(0,len(result)):
  f.write(result[i]+'\n')
f.close()

import pandas as pd

twitter=pd.read_excel('/content/Arabic_Tweets.xlsx')

twitter["english"]=0

twitter.to_csv('twitter2.csv')

for i in range(0,len(twitter['توییت'])):
  twitter["english"][i]=non_arabic(preprocessing(twitter['توییت'][i]))

"""#Tokenization:

"""

!pip install pyarabic

from pyarabic.araby import tokenize, is_arabicrange
def tokenize_non_arabic(text):
  token=tokenize(text, conditions=is_arabicrange)
  return token

# def remove_stop_words2(token):
#     t_r=[]
#     # zen = TextBlob(text)
#     # words = zen.words
#     for w in token:
#        if w not in stopwords_set and len(w) >= 2:
#         t_r.append(w)
#     return t_r

#finding and removing non-arabic texts:
def remove_non_arabic(text):
  arabic_text=''
  for ele in text:
    if re.findall("[^\u0000-\u05C0\u2100-\u214F]+", ele):
      arabic_text=arabic_text+ele
  return arabic_text

  # english = [idx for idx in token if not re.findall("[^\u0000-\u05C0\u2100-\u214F]+", idx)]
  # return  ''.join(idx for idx in token if not re.findall("[^\u0000-\u05C0\u2100-\u214F]+", idx))

!pip install camel_tools

from camel_tools.tokenizers.morphological import MorphologicalTokenizer
from camel_tools.tokenizers.word import simple_word_tokenize
from camel_tools.disambig.mle import MLEDisambiguator

# Initialize MorphologicalTokenizer
mle = MLEDisambiguator.pretrained('calima-msa-r13')
tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True)
def tokenize(sentence):
    # Tokenize the sentence
    sentence = simple_word_tokenize(sentence)
    tokens = tokenizer.tokenize(sentence)
    return tokens

"""#POS_tagger:"""

import nltk
nltk.download('averaged_perceptron_tagger')
from nltk.tag import pos_tag

!cp '/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition/stanford-tagger-4.2.0.zip' '/content'

!unzip '/content/stanford-tagger-4.2.0.zip'

#دقیقتر از nltk
import nltk.tag.stanford as st
#  import POSTagger
path_to_model='/content/stanford-postagger-full-2020-11-17/models/arabic.tagger'
# artagger = POSTagger(path_to_model, encoding='utf8')
tagger = st.StanfordPOSTagger(path_to_model, '/content/stanford-postagger-full-2020-11-17/stanford-postagger.jar')

tagger._SEPARATOR = '/'
tagged_sent = tagger.tag(tokens)
tagged_sent

pos_tag(tokens)

"""#Stemming:

ISRIStemmer--> root good for verbs<br>
snowball stememr--> light stem for nouns<br>
PorterStemmer-->not for arabic
light stemmer--> best
"""

from nltk import ISRIStemmer
def stemming(tokens):
    stemmer = ISRIStemmer()
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    return stemmed_tokens
stemming(tokens)

"""SnowballStemmer"""

# import nltk
# from nltk.stem.snowball import SnowballStemmer
# snow_stemmer = SnowballStemmer(language='arabic')

stem_words = []
for w in tokens:
    x = snow_stemmer.stem(w)
    stem_words.append(x)

for e1,e2 in zip(tokens,stem_words):
    print(e1+' ----> '+e2)

"""light_stemmer"""

pip install tashaphyne

from tashaphyne.stemming import ArabicLightStemmer
ArListem = ArabicLightStemmer()
def light_stemmer(tokens):
  stem_words = []
  for w in tokens:
      stem = ArListem.light_stem(w)
      stem_words.append(stem)
  return stem_words

light_stemmer(tokens)

text='🔻بالصور وقفة في كفرياسيف بالداخل المحتل، تنديداً باستمرار عدوان الإحتلال على قطاع غزة. #رفح_تباد_يا_امه_المليار #رفح_تُباد_يا_أمة_المليار https://t.co/jh9CHDncpm '
text=remove_stop_words(normalizing(text))
tokens=tokenize(text)

text='بالصور وقفه كفرياسيف بالداخل المحتل تنديدا باستمرار عدوان الاحتلال قطاع غزه رفح تباد امه المليار رفح تباد امه المليار'

tokens=['بالصور','وقفه','كفرياسيف','بالداخل','المحتل','تنديدا','باستمرار','عدوان','الاحتلال','قطاع','غزه','رفح','تباد','امه','المليار','رفح','تباد','امه','المليار']

"""farasa"""

from farasa.stemmer import FarasaStemmer

stemmer = FarasaStemmer()

stemmer_interactive = FarasaStemmer(interactive=True)

stemmed_interactive = stemmer_interactive.stem(text)
print("sample stemmed (interactive):",stemmed_interactive)
# terminate the object to save resources:
# stemmer_interactive.terminate()

"""#Word Embedding"""

text='🔻بالصور وقفة في كفرياسيف بالداخل المحتل، تنديداً باستمرار عدوان الإحتلال على قطاع غزة. #رفح_تباد_يا_امه_المليار #رفح_تُباد_يا_أمة_المليار https://t.co/jh9CHDncpm '
text=remove_stop_words(normalizing(text))
tokens=tokenize(text)

"""###aravec an arabic word2vec with gensim:"""

import tensorflow as tf
from keras.preprocessing.text import Tokenizer

from keras.preprocessing.sequence import pad_sequences
from numpy import array

import gensim
from gensim.models import KeyedVectors
from gensim.models import word2vec

text='بالصور وقفه كفرياسيف بالداخل المحتل تنديدا باستمرار عدوان الاحتلال قطاع غزه رفح تباد امه المليار رفح تباد امه المليار'
tokens=['بالصور','وقفه','كفرياسيف','بالداخل','المحتل','تنديدا','باستمرار','عدوان','الاحتلال','قطاع','غزه','رفح','تباد','امه','المليار','رفح','تباد','امه','المليار']

docs=[ "كان هذا يوم سعيد","ماشاءهللا  هذا عمل جيد"]

vocab_size

def merge_list(listoftokens):
  merged=[]
  for tokens in listoftokens:
    merged=merged+tokens
    return merged

from collections import Counter
def word_index(listoftokens):
  merged=merge_list(listoftokens)
  vocab_size=len(Counter(merged).keys())+1
  return vocab_size

# t = Tokenizer()
# t.fit_on_texts(docs)
# vocab_size = len(t.word_index) + 1
# integer encode the documents
encoded_docs = t.texts_to_sequences(docs)
print("encoded_docs:\n",encoded_docs)
# pad documents to a max length of 4 words
max_length = 100
padded_docs = pad_sequences(encoded_docs, maxlen=max_length,
padding="post")
print("padded_docs:\n",padded_docs)

encoded_docs = t.texts_to_sequences(['docs','hab'])
encoded_docs

!unzip  '/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition/tweet_cbow_100.zip'  -d '/content'

# load the whole embedding into memory
w2v_embeddings_index={}
TOTAL_EMBEDDING_DIM=100
embeddings_file="/content/tweets_cbow_100"
# w2v_model =KeyedVectors.load(embeddings_file)
for word in w2v_model.wv.key_to_index :
    w2v_embeddings_index[word] = w2v_model.wv.key_to_index[word]
print("Loaded %s word vectors."% len(w2v_embeddings_index))

import numpy as np

# create a weight matrix for words in training docs
embedding_matrix = np.zeros((vocab_size, TOTAL_EMBEDDING_DIM))
for word, i in t.word_index.items():
    embedding_vector = w2v_embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
print("Embedding Matrix shape:", embedding_matrix.shape)

embedding_layer = tf.keras.layers.Embedding(vocab_size, TOTAL_EMBEDDING_DIM, weights=[embedding_matrix], input_length=4, trainable=False)

!git clone https://huggingface.co/datasets/emotone-ar-cicling2017/emotone_ar

!wget 'https://raw.githubusercontent.com/AmrMehasseb/Emotional-Tone/master/Emotional-Tone-Dataset.csv'

!cp '/content/Emotional-Tone-Dataset.csv' '/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition'

!wget 'https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/d9yy8w52ns-2.zip'

!unzip '/content/ArPanEmo An Open-Source Dataset for Fine-Grained Emotion Recognition in Arabic Online Content during COVID-19 Pandemic/ArPanEmo.zip'

#aranet emo model
!gdown '1D1nvw715Yp_yK6XYYPfUxnxygfPEEK2k'

import tarfile
file = tarfile.open('/content/emotion_aranet.tar.gz')
print(file.getnames())
file.extractall('content')
file.close()

!cp -r '/content/ArPanEmo' '/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition'

"""###aravec with spacy"""

!wget 'https://archive.org/download/aravec2.0/tweet_cbow_100.zip'

!cp 'tweet_cbow_100.zip' '/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition'
!unzip 'tweet_cbow_100.zip'

import gensim
import spacy
import nltk

# %mkdir spacyModel
# model = gensim.models.Word2Vec.load("/content/tweets_cbow_100")
print("We've",len(model.wv.index_to_key),"vocabularies")
model.wv.save_word2vec_format("./spacyModel/aravec.txt")
!gzip ./spacyModel/aravec.txt

!python -m spacy init vectors ar ./spacyModel/aravec.txt.gz spacy.aravec.model

!cp -r '/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition/spacy.aravec.model' '/content'

# prefer_gpu()
nlp = spacy.load("./spacy.aravec.model/")
nlp("قطة").vector

# Define the preprocessing Class
class Preprocessor:
    def __init__(self, tokenizer, **cfg):
        self.tokenizer = tokenizer

    def __call__(self, text):
        preprocessed = clean_str(text)
        return self.tokenizer(preprocessed)

nlp.tokenizer = Preprocessor(nlp.tokenizer)

# adapted from https://github.com/bakrianoo/aravec
# Load AraVec model from gensim
model = gensim.models.Word2Vec.load("full_grams_cbow_100_twitter.mdl")

# export to word2vec format for spacy model
model.wv.save_word2vec_format("./spacyModel/aravec.txt")
# run the following command gzip ./spacyModel/aravec.txt
# run the following command python -m spacy  init-model ar spacy.aravec.model --vectors-loc ./spacyModel/aravec.txt.gz

# load spacy model
nlp = spacy.load("./spacy.aravec.model/"
view rawAraVec_spacy_model.py hosted wit

# load the AraVec model
model = gensim.models.Word2Vec.load("full_grams_cbow_100_twitter.mdl")
print("We've",len(model.wv.index2word),"vocabularies")

"""#DL-Models:"""

# define model
input_placeholder= tf.keras.Input(shape=(4,), dtype=’int32')
input_embedding = embedding_layer(input_placeholder)
lstm= tf.keras.layers.LSTM(units=10, activation=’relu’)(input_embedding)
preds = tf.keras.layers.Dense(1, activation=’sigmoid’, name = “activation”)(lstm)
model = tf.keras.models.Model(inputs=input_placeholder, outputs=preds)
# compile the model
model.compile(loss=’binary_crossentropy’, optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999), metrics=[‘accuracy’])
# summarize the model
print(model.summary())
print(‘\n # Fit model on training data’)
# fit the model
history= model.fit(padded_docs, labels, epochs=50, verbose=0)

"""###Evaluation"""

# evaluate the model
loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)
print(‘Accuracy: %f’ % (accuracy*100))

"""###Test"""

text=[‘عمل جيد’]
encoded_text = t.texts_to_sequences(text)
print(‘encoded_text:\n’,encoded_text)
# pad documents to a max length of 4 words
padded_text = pad_sequences(encoded_text, maxlen=max_length, padding=’post’)
print(‘padded_text:\n’,padded_text)
result=int(model.predict(padded_text).round().item())
print(‘Input %s \n Prediction: %s’ %(text,result))

"""#Other materials:

##Farasa NER:
"""

pip install farasapy

from farasa.ner import FarasaNamedEntityRecognizer
named_entity_recognizer = FarasaNamedEntityRecognizer()

named_entity_recognized = named_entity_recognizer.recognize('غزه')
print("sample named entity recognized:",named_entity_recognized)

def ner(token):
  named_entity_recognized = named_entity_recognizer.recognize(token)
  return named_entity_recognized

def stemmer(tokens):
  stem_list=[]
  entities=[]
  for token in tokens:
    if ner(token) in entities:
      stem_list.append(token)
    else:
      stem_list.append(stem(token))

"""##Farasa Stemmer: take long time, more accurate(5%)"""

from farasa.stemmer import FarasaStemmer

stemmer = FarasaStemmer()

stemmer.stem("مليار" )