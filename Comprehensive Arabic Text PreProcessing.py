# -*- coding: utf-8 -*-
"""Emotion Recognition in Arabic Texts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fpG2vcyh2MjTiRqs3VpPqHSA_llj38Gn
"""

import re
import string

"""should we remove # words? <br>
should we remove english & non-ararbic(chi-jap) words? yes<br>
improve remove_digits() ok<br>
save_emojis: ok<br>
  Ùˆ Ø¶Ù…Ø§ÛŒØ±ÛŒ Ù…Ø§Ù†Ù†Ø¯ Ú©Ù… Ø§Ø² Ø¬Ù…Ù„Ù‡: ok
  <br>
Ø­Ø°Ù ÙØ§ØµÙ„Ù‡

#Normalization
"""

!pip install emoji==1.7
import emoji

def is_emoji(s):
  emojis=''
  for c in s:
    if c in emoji.UNICODE_EMOJI['en'] and c not in emojis:
      emojis=emojis+c
  # return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'] and c )
  return emojis

# This function modifies caption text and prepare it for key extraction:
def remove_emoji(text):
    #removing Imojis:
    emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                              "]+", flags=re.UNICODE)
    without_emoji=emoji_pattern.sub(r'',text)

    emojis=''
    for c in text:
      if c in emoji.UNICODE_EMOJI['en'] and c not in emojis:
       emojis=emojis+c
    return without_emoji,emojis


def  remove_id(text):
    #Removing IDs:
    without_id = re.sub('@[^\s]+','',text)
    return without_id
    #Finding Hashtags in caption of post:
    # tags=[]
    # for tag in without_id.split():
    #   if tag.startswith("#"):
    #     tags.append(tag.strip("#").replace("_", " "))

    #Removing # sign and _:

def remove_hashtags(text):
    without_tag=text.replace('#','').replace("_", " ")
    return  without_tag

def remove_links(text):
    link = re.compile(r'https?://\S+|www\.\S+')
    text_without_link = link.sub('', text)

    return text_without_link

def remove_digits(text):
  # special_digits=['0','20','100']
  non_digit=''
  for i in text:
    if not i.isdigit():
      non_digit=non_digit+i
    # elif i in special_digits:
    #   non_digit=non_digit+i
  return non_digit

def remove_html_tags(text):
  cleaner = re.compile('<.*?>')
  without_tag = re.sub(cleaner, '', text)
  result = without_tag.replace(u'\xa0', u' ')
  return result

arabic_punctuations = '''`Ã·Ã—Â«Â»Ø›<>_()*&^%][Ù€ØŒ/:"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€'''
english_punctuations = string.punctuation
punctuations_list = arabic_punctuations + english_punctuations

arabic_diacritics = re.compile("""
                             Ù‘    | # Tashdid
                             Ù    | # Fatha
                             Ù‹    | # Tanwin Fath
                             Ù    | # Damma
                             ÙŒ    | # Tanwin Damm
                             Ù    | # Kasra
                             Ù    | # Tanwin Kasr
                             Ù’    | # Sukun
                             Ù€     # Tatwil/Kashida

                         """, re.VERBOSE)


def normalize_arabic(text):
    text = re.sub("[Ø¥Ø£Ø¢Ø§]", "Ø§", text)
    text = re.sub("Ù‰", "ÙŠ", text)
    text = re.sub("Ø¤", "Ø¡", text)
    text = re.sub("Ø¦", "Ø¡", text)
    text = re.sub("Ø©", "Ù‡", text)
    text = re.sub("Ú¯", "Ùƒ", text)
    return text


def remove_diacritics(text):
    text = re.sub(arabic_diacritics, '', text)
    return text


def remove_punctuations(text):
    translator = str.maketrans('', '', punctuations_list)
    return text.translate(translator)

# Investigating:
def remove_repeating_char(text):
    return re.sub(r'(.)\1+', r'\1', text)
#ham digit ro hazf mikone ham 2 harf tekrari ro

#Removing  elongation:
def remove_elongation(text):
    text = re.sub('Ù€', '', text)
    return text

# Removing Unicodes:
def remove_unicode(text):
    text.encode('ascii', errors='ignore')
    return text

def normalizing(text):
  without_emoji,emoji_list=remove_emoji(text)
  without_id=remove_id(without_emoji)
  without_hashtag=remove_hashtags(without_id)
  without_html_tags=remove_html_tags(without_hashtag)
  without_link=remove_links(without_html_tags)
  without_digits=remove_digits(without_link)
  without_diacritics= remove_diacritics(without_digits)
  without_punctuation=remove_punctuations(without_diacritics)
  without_elongation=remove_elongation(without_punctuation)
  without_unicode=remove_unicode(without_elongation)
  normalized=normalize_arabic(without_unicode)
  return normalized

#Spell Checking:
#need to investigating other models
!pip install ar-corrector
from ar_corrector.corrector import Corrector

corrector = Corrector()
text = 'ğŸ”»Ø¨Ø§Ù„ØµÙˆØ± ÙˆÙ‚ÙØ© ÙÙŠ ÙƒÙØ±ÙŠØ§Ø³ÙŠÙ Ø¨Ø§Ù„Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ„ØŒ ØªÙ†Ø¯ÙŠØ¯Ø§Ù‹ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø¹Ø¯ÙˆØ§Ù† Ø§Ù„Ø¥Ø­ØªÙ„Ø§Ù„ Ø¹Ù„Ù‰ Ù‚Ø·Ø§Ø¹ ØºØ²Ø©. Ø±ÙØ­ ØªØ¨Ø§Ø¯ ÙŠØ§ Ø§Ù…Ù‡ Ø§Ù„Ù…Ù„ÙŠØ§Ø±'
print(corrector.contextual_correct(text))

"""#Removing StopWords:"""

#Removing Stop words:
# from nltk.corpus import stopwords
import nltk
# nltk.download('stopwords')
nltk.download('punkt')
from textblob import TextBlob

with open('/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition/combiation_sw2_nonemotional_normalized&rmTashkeel2','r') as file2:
  stopwords=[]
  line2=file2.readline().replace('\n','')
  stopwords.append(line2)

  while line2:
    line2=file2.readline().replace('\n','')
    stopwords.append(line2)

stopwords.remove('')
stopwords_set = set(stopwords)

def remove_stop_words(text):
    zen = TextBlob(text)
    words = zen.words
    return " ".join([w for w in words if not w in stopwords_set and len(w) >= 2])

without_stopwords=remove_stop_words(preprocessing('Ø£ÙŠÙ‘Ù‡Ø§ Ø§Ù„Ø´Ø¨Ø§Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠÙ‘ÙˆÙ† ÙÙŠ Ø£Ù…Ø±ÙŠÙƒØ§!Ù‚Ø¯ Ø´Ø±Ø¹ØªÙ… Ø¨Ù†Ø¶Ø§Ù„Ù Ø´Ø±ÙŠÙÙ ØªØ­Øª Ø¶ØºÙˆØ· Ø­ÙƒÙˆÙ…ØªÙƒÙ… Ø§Ù„Ù‚Ø§Ø³ÙŠØ©ØŒ Ø§Ù„ØªÙŠ ØªØ¯Ø§ÙØ¹ Ø¹Ù† Ø§Ù„ÙƒÙŠØ§Ù† Ø§Ù„ØµÙ€Ù€Ù€Ù€.Ù€.Ù‡ÙŠÙˆÙ†ÙŠ. Ø§Ù„Ø¥Ù…Ø§Ù… Ø§Ù„Ø³ÙŠØ¯ Ø¹Ù„ÙŠ Ø§Ù„Ø®Ø§Ù…Ù†Ø¦ÙŠ (Ø¯Ø§Ù… Ø¸Ù„Ù‡) ÙÙŠ Ø±Ø³Ø§Ù„Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø·Ù„Ù‘Ø§Ø¨ Ø§Ù„Ø¬Ø§Ù…Ø¹ÙŠÙ‘ÙŠÙ† ÙˆØ§Ù„Ø´Ø¨Ø§Ø¨ ÙÙŠ Ø§Ù„ÙˆÙ„Ø§ÙŠØ§Øª Ø§Ù„Ù…ØªÙ‘Ø­Ø¯Ø© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠÙ‘Ø© 25/05/2024 https://t.co/4W6wtVpjFwÂ '))

stopwords_set = set(stopwords)
stopwords2_set = set(st_t_norm)
in_second_but_not_in_first = stopwords2_set - stopwords_set
result = stopwords + list(in_second_but_not_in_first)
len(result)

f=open('/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition/combiation_sw2_nonemotional_normalized&rmTashkeel2','w')
for i in range(0,len(result)):
  f.write(result[i]+'\n')
f.close()

import pandas as pd

twitter=pd.read_excel('/content/Arabic_Tweets.xlsx')

twitter["english"]=0

twitter.to_csv('twitter2.csv')

for i in range(0,len(twitter['ØªÙˆÛŒÛŒØª'])):
  twitter["english"][i]=non_arabic(preprocessing(twitter['ØªÙˆÛŒÛŒØª'][i]))

"""#Tokenization:

"""

!pip install pyarabic

from pyarabic.araby import tokenize, is_arabicrange
def tokenize_non_arabic(text):
  token=tokenize(text, conditions=is_arabicrange)
  return token

# def remove_stop_words2(token):
#     t_r=[]
#     # zen = TextBlob(text)
#     # words = zen.words
#     for w in token:
#        if w not in stopwords_set and len(w) >= 2:
#         t_r.append(w)
#     return t_r

#finding and removing non-arabic texts:
def remove_non_arabic(text):
  arabic_text=''
  for ele in text:
    if re.findall("[^\u0000-\u05C0\u2100-\u214F]+", ele):
      arabic_text=arabic_text+ele
  return arabic_text

  # english = [idx for idx in token if not re.findall("[^\u0000-\u05C0\u2100-\u214F]+", idx)]
  # return  ''.join(idx for idx in token if not re.findall("[^\u0000-\u05C0\u2100-\u214F]+", idx))

!pip install camel_tools

from camel_tools.tokenizers.morphological import MorphologicalTokenizer
from camel_tools.tokenizers.word import simple_word_tokenize
from camel_tools.disambig.mle import MLEDisambiguator

# Initialize MorphologicalTokenizer
mle = MLEDisambiguator.pretrained('calima-msa-r13')
tokenizer = MorphologicalTokenizer(mle, scheme='d3tok', split=True)
def tokenize(sentence):
    # Tokenize the sentence
    sentence = simple_word_tokenize(sentence)
    tokens = tokenizer.tokenize(sentence)
    return tokens

"""#POS_tagger:"""

import nltk
nltk.download('averaged_perceptron_tagger')
from nltk.tag import pos_tag

!cp '/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition/stanford-tagger-4.2.0.zip' '/content'

!unzip '/content/stanford-tagger-4.2.0.zip'

#Ø¯Ù‚ÛŒÙ‚ØªØ± Ø§Ø² nltk
import nltk.tag.stanford as st
#  import POSTagger
path_to_model='/content/stanford-postagger-full-2020-11-17/models/arabic.tagger'
# artagger = POSTagger(path_to_model, encoding='utf8')
tagger = st.StanfordPOSTagger(path_to_model, '/content/stanford-postagger-full-2020-11-17/stanford-postagger.jar')

tagger._SEPARATOR = '/'
tagged_sent = tagger.tag(tokens)
tagged_sent

pos_tag(tokens)

"""#Stemming:

ISRIStemmer--> root good for verbs<br>
snowball stememr--> light stem for nouns<br>
PorterStemmer-->not for arabic
light stemmer--> best
"""

from nltk import ISRIStemmer
def stemming(tokens):
    stemmer = ISRIStemmer()
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    return stemmed_tokens
stemming(tokens)

"""SnowballStemmer"""

# import nltk
# from nltk.stem.snowball import SnowballStemmer
# snow_stemmer = SnowballStemmer(language='arabic')

stem_words = []
for w in tokens:
    x = snow_stemmer.stem(w)
    stem_words.append(x)

for e1,e2 in zip(tokens,stem_words):
    print(e1+' ----> '+e2)

"""light_stemmer"""

pip install tashaphyne

from tashaphyne.stemming import ArabicLightStemmer
ArListem = ArabicLightStemmer()
def light_stemmer(tokens):
  stem_words = []
  for w in tokens:
      stem = ArListem.light_stem(w)
      stem_words.append(stem)
  return stem_words

light_stemmer(tokens)

text='ğŸ”»Ø¨Ø§Ù„ØµÙˆØ± ÙˆÙ‚ÙØ© ÙÙŠ ÙƒÙØ±ÙŠØ§Ø³ÙŠÙ Ø¨Ø§Ù„Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ„ØŒ ØªÙ†Ø¯ÙŠØ¯Ø§Ù‹ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø¹Ø¯ÙˆØ§Ù† Ø§Ù„Ø¥Ø­ØªÙ„Ø§Ù„ Ø¹Ù„Ù‰ Ù‚Ø·Ø§Ø¹ ØºØ²Ø©. #Ø±ÙØ­_ØªØ¨Ø§Ø¯_ÙŠØ§_Ø§Ù…Ù‡_Ø§Ù„Ù…Ù„ÙŠØ§Ø± #Ø±ÙØ­_ØªÙØ¨Ø§Ø¯_ÙŠØ§_Ø£Ù…Ø©_Ø§Ù„Ù…Ù„ÙŠØ§Ø± https://t.co/jh9CHDncpmÂ '
text=remove_stop_words(normalizing(text))
tokens=tokenize(text)

text='Ø¨Ø§Ù„ØµÙˆØ± ÙˆÙ‚ÙÙ‡ ÙƒÙØ±ÙŠØ§Ø³ÙŠÙ Ø¨Ø§Ù„Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ„ ØªÙ†Ø¯ÙŠØ¯Ø§ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø¹Ø¯ÙˆØ§Ù† Ø§Ù„Ø§Ø­ØªÙ„Ø§Ù„ Ù‚Ø·Ø§Ø¹ ØºØ²Ù‡ Ø±ÙØ­ ØªØ¨Ø§Ø¯ Ø§Ù…Ù‡ Ø§Ù„Ù…Ù„ÙŠØ§Ø± Ø±ÙØ­ ØªØ¨Ø§Ø¯ Ø§Ù…Ù‡ Ø§Ù„Ù…Ù„ÙŠØ§Ø±'

tokens=['Ø¨Ø§Ù„ØµÙˆØ±','ÙˆÙ‚ÙÙ‡','ÙƒÙØ±ÙŠØ§Ø³ÙŠÙ','Ø¨Ø§Ù„Ø¯Ø§Ø®Ù„','Ø§Ù„Ù…Ø­ØªÙ„','ØªÙ†Ø¯ÙŠØ¯Ø§','Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø±','Ø¹Ø¯ÙˆØ§Ù†','Ø§Ù„Ø§Ø­ØªÙ„Ø§Ù„','Ù‚Ø·Ø§Ø¹','ØºØ²Ù‡','Ø±ÙØ­','ØªØ¨Ø§Ø¯','Ø§Ù…Ù‡','Ø§Ù„Ù…Ù„ÙŠØ§Ø±','Ø±ÙØ­','ØªØ¨Ø§Ø¯','Ø§Ù…Ù‡','Ø§Ù„Ù…Ù„ÙŠØ§Ø±']

"""farasa"""

from farasa.stemmer import FarasaStemmer

stemmer = FarasaStemmer()

stemmer_interactive = FarasaStemmer(interactive=True)

stemmed_interactive = stemmer_interactive.stem(text)
print("sample stemmed (interactive):",stemmed_interactive)
# terminate the object to save resources:
# stemmer_interactive.terminate()

"""#Word Embedding"""

text='ğŸ”»Ø¨Ø§Ù„ØµÙˆØ± ÙˆÙ‚ÙØ© ÙÙŠ ÙƒÙØ±ÙŠØ§Ø³ÙŠÙ Ø¨Ø§Ù„Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ„ØŒ ØªÙ†Ø¯ÙŠØ¯Ø§Ù‹ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø¹Ø¯ÙˆØ§Ù† Ø§Ù„Ø¥Ø­ØªÙ„Ø§Ù„ Ø¹Ù„Ù‰ Ù‚Ø·Ø§Ø¹ ØºØ²Ø©. #Ø±ÙØ­_ØªØ¨Ø§Ø¯_ÙŠØ§_Ø§Ù…Ù‡_Ø§Ù„Ù…Ù„ÙŠØ§Ø± #Ø±ÙØ­_ØªÙØ¨Ø§Ø¯_ÙŠØ§_Ø£Ù…Ø©_Ø§Ù„Ù…Ù„ÙŠØ§Ø± https://t.co/jh9CHDncpmÂ '
text=remove_stop_words(normalizing(text))
tokens=tokenize(text)

"""###aravec an arabic word2vec with gensim:"""

import tensorflow as tf
from keras.preprocessing.text import Tokenizer

from keras.preprocessing.sequence import pad_sequences
from numpy import array

import gensim
from gensim.models import KeyedVectors
from gensim.models import word2vec

text='Ø¨Ø§Ù„ØµÙˆØ± ÙˆÙ‚ÙÙ‡ ÙƒÙØ±ÙŠØ§Ø³ÙŠÙ Ø¨Ø§Ù„Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…Ø­ØªÙ„ ØªÙ†Ø¯ÙŠØ¯Ø§ Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø¹Ø¯ÙˆØ§Ù† Ø§Ù„Ø§Ø­ØªÙ„Ø§Ù„ Ù‚Ø·Ø§Ø¹ ØºØ²Ù‡ Ø±ÙØ­ ØªØ¨Ø§Ø¯ Ø§Ù…Ù‡ Ø§Ù„Ù…Ù„ÙŠØ§Ø± Ø±ÙØ­ ØªØ¨Ø§Ø¯ Ø§Ù…Ù‡ Ø§Ù„Ù…Ù„ÙŠØ§Ø±'
tokens=['Ø¨Ø§Ù„ØµÙˆØ±','ÙˆÙ‚ÙÙ‡','ÙƒÙØ±ÙŠØ§Ø³ÙŠÙ','Ø¨Ø§Ù„Ø¯Ø§Ø®Ù„','Ø§Ù„Ù…Ø­ØªÙ„','ØªÙ†Ø¯ÙŠØ¯Ø§','Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø±','Ø¹Ø¯ÙˆØ§Ù†','Ø§Ù„Ø§Ø­ØªÙ„Ø§Ù„','Ù‚Ø·Ø§Ø¹','ØºØ²Ù‡','Ø±ÙØ­','ØªØ¨Ø§Ø¯','Ø§Ù…Ù‡','Ø§Ù„Ù…Ù„ÙŠØ§Ø±','Ø±ÙØ­','ØªØ¨Ø§Ø¯','Ø§Ù…Ù‡','Ø§Ù„Ù…Ù„ÙŠØ§Ø±']

docs=[ "ÙƒØ§Ù† Ù‡Ø°Ø§ ÙŠÙˆÙ… Ø³Ø¹ÙŠØ¯","Ù…Ø§Ø´Ø§Ø¡Ù‡Ù„Ù„Ø§  Ù‡Ø°Ø§ Ø¹Ù…Ù„ Ø¬ÙŠØ¯"]

vocab_size

def merge_list(listoftokens):
  merged=[]
  for tokens in listoftokens:
    merged=merged+tokens
    return merged

from collections import Counter
def word_index(listoftokens):
  merged=merge_list(listoftokens)
  vocab_size=len(Counter(merged).keys())+1
  return vocab_size

# t = Tokenizer()
# t.fit_on_texts(docs)
# vocab_size = len(t.word_index) + 1
# integer encode the documents
encoded_docs = t.texts_to_sequences(docs)
print("encoded_docs:\n",encoded_docs)
# pad documents to a max length of 4 words
max_length = 100
padded_docs = pad_sequences(encoded_docs, maxlen=max_length,
padding="post")
print("padded_docs:\n",padded_docs)

encoded_docs = t.texts_to_sequences(['docs','hab'])
encoded_docs

!unzip  '/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition/tweet_cbow_100.zip'  -d '/content'

# load the whole embedding into memory
w2v_embeddings_index={}
TOTAL_EMBEDDING_DIM=100
embeddings_file="/content/tweets_cbow_100"
# w2v_model =KeyedVectors.load(embeddings_file)
for word in w2v_model.wv.key_to_index :
    w2v_embeddings_index[word] = w2v_model.wv.key_to_index[word]
print("Loaded %s word vectors."% len(w2v_embeddings_index))

import numpy as np

# create a weight matrix for words in training docs
embedding_matrix = np.zeros((vocab_size, TOTAL_EMBEDDING_DIM))
for word, i in t.word_index.items():
    embedding_vector = w2v_embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
print("Embedding Matrix shape:", embedding_matrix.shape)

embedding_layer = tf.keras.layers.Embedding(vocab_size, TOTAL_EMBEDDING_DIM, weights=[embedding_matrix], input_length=4, trainable=False)

!git clone https://huggingface.co/datasets/emotone-ar-cicling2017/emotone_ar

!wget 'https://raw.githubusercontent.com/AmrMehasseb/Emotional-Tone/master/Emotional-Tone-Dataset.csv'

!cp '/content/Emotional-Tone-Dataset.csv' '/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition'

!wget 'https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/d9yy8w52ns-2.zip'

!unzip '/content/ArPanEmo An Open-Source Dataset for Fine-Grained Emotion Recognition in Arabic Online Content during COVID-19 Pandemic/ArPanEmo.zip'

#aranet emo model
!gdown '1D1nvw715Yp_yK6XYYPfUxnxygfPEEK2k'

import tarfile
file = tarfile.open('/content/emotion_aranet.tar.gz')
print(file.getnames())
file.extractall('content')
file.close()

!cp -r '/content/ArPanEmo' '/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition'

"""###aravec with spacy"""

!wget 'https://archive.org/download/aravec2.0/tweet_cbow_100.zip'

!cp 'tweet_cbow_100.zip' '/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition'
!unzip 'tweet_cbow_100.zip'

import gensim
import spacy
import nltk

# %mkdir spacyModel
# model = gensim.models.Word2Vec.load("/content/tweets_cbow_100")
print("We've",len(model.wv.index_to_key),"vocabularies")
model.wv.save_word2vec_format("./spacyModel/aravec.txt")
!gzip ./spacyModel/aravec.txt

!python -m spacy init vectors ar ./spacyModel/aravec.txt.gz spacy.aravec.model

!cp -r '/content/drive/MyDrive/ElmOSanat_qwe/2-Emotion Recognition/spacy.aravec.model' '/content'

# prefer_gpu()
nlp = spacy.load("./spacy.aravec.model/")
nlp("Ù‚Ø·Ø©").vector

# Define the preprocessing Class
class Preprocessor:
    def __init__(self, tokenizer, **cfg):
        self.tokenizer = tokenizer

    def __call__(self, text):
        preprocessed = clean_str(text)
        return self.tokenizer(preprocessed)

nlp.tokenizer = Preprocessor(nlp.tokenizer)

# adapted from https://github.com/bakrianoo/aravec
# Load AraVec model from gensim
model = gensim.models.Word2Vec.load("full_grams_cbow_100_twitter.mdl")

# export to word2vec format for spacy model
model.wv.save_word2vec_format("./spacyModel/aravec.txt")
# run the following command gzip ./spacyModel/aravec.txt
# run the following command python -m spacy  init-model ar spacy.aravec.model --vectors-loc ./spacyModel/aravec.txt.gz

# load spacy model
nlp = spacy.load("./spacy.aravec.model/"
view rawAraVec_spacy_model.py hosted wit

# load the AraVec model
model = gensim.models.Word2Vec.load("full_grams_cbow_100_twitter.mdl")
print("We've",len(model.wv.index2word),"vocabularies")

"""#DL-Models:"""

# define model
input_placeholder= tf.keras.Input(shape=(4,), dtype=â€™int32')
input_embedding = embedding_layer(input_placeholder)
lstm= tf.keras.layers.LSTM(units=10, activation=â€™reluâ€™)(input_embedding)
preds = tf.keras.layers.Dense(1, activation=â€™sigmoidâ€™, name = â€œactivationâ€)(lstm)
model = tf.keras.models.Model(inputs=input_placeholder, outputs=preds)
# compile the model
model.compile(loss=â€™binary_crossentropyâ€™, optimizer=tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999), metrics=[â€˜accuracyâ€™])
# summarize the model
print(model.summary())
print(â€˜\n # Fit model on training dataâ€™)
# fit the model
history= model.fit(padded_docs, labels, epochs=50, verbose=0)

"""###Evaluation"""

# evaluate the model
loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)
print(â€˜Accuracy: %fâ€™ % (accuracy*100))

"""###Test"""

text=[â€˜Ø¹Ù…Ù„ Ø¬ÙŠØ¯â€™]
encoded_text = t.texts_to_sequences(text)
print(â€˜encoded_text:\nâ€™,encoded_text)
# pad documents to a max length of 4 words
padded_text = pad_sequences(encoded_text, maxlen=max_length, padding=â€™postâ€™)
print(â€˜padded_text:\nâ€™,padded_text)
result=int(model.predict(padded_text).round().item())
print(â€˜Input %s \n Prediction: %sâ€™ %(text,result))

"""#Other materials:

##Farasa NER:
"""

pip install farasapy

from farasa.ner import FarasaNamedEntityRecognizer
named_entity_recognizer = FarasaNamedEntityRecognizer()

named_entity_recognized = named_entity_recognizer.recognize('ØºØ²Ù‡')
print("sample named entity recognized:",named_entity_recognized)

def ner(token):
  named_entity_recognized = named_entity_recognizer.recognize(token)
  return named_entity_recognized

def stemmer(tokens):
  stem_list=[]
  entities=[]
  for token in tokens:
    if ner(token) in entities:
      stem_list.append(token)
    else:
      stem_list.append(stem(token))

"""##Farasa Stemmer: take long time, more accurate(5%)"""

from farasa.stemmer import FarasaStemmer

stemmer = FarasaStemmer()

stemmer.stem("Ù…Ù„ÙŠØ§Ø±" )